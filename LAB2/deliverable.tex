\documentclass[12]{article}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\author{par4111 \\ Adri√† Cabeza, Xavier Lacasa \\ Departament d' Arquitectura de Computadors}
\title{Lab 2: Brief tutorial on OpenMP programming model }
\date{\today \\ 2018 - 19 PRIMAVERA}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\lstset{
  basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{orange},
  keywordstyle=\color{blue},
	frame=tb,language=C,breaklines=true,numbers=none,  stringstyle=\color{red}, tabsize=3,   showstringspaces=false,
  columns=flexible, 
}
\begin{document}
\maketitle

\newpage
\tableofcontents
\newpage
\section{Introduction}

In this laboratory assignment we will learn the basics of OpenMP extensions to the C programming languge. At this moment we already know the hardware and tools we are going to use but this is the first approach to learn about the software needed to parallelize real applications. 

This is the scope of the next two sessions of PAR laboratory: introducing the  main components of the OpenMP programming model, from the simpler to the most complicated pragmas. 

\section{OpenMP questionnaire}

\subsection{Parallel regions}
\subsubsection{1.hello.c}
\textbf{1. How many times will you see the "Hello world!" message if the program is executed with \textit{./1.hello}?}

We see the message 24 times. This is due to the \textit{\#pragma omp parallel} call before \textit{printf("Hello world! \textbackslash n");}, which makes every available thread execute the \textit{printf}. In Boada 1, they happen to be 24 threads, and so the message is printed 24 times.

\textbf{2. Without changing the program, how to make it to print 4 times the \textit{Hello World!} message?}

By setting the number of threads available to 4, only 4 threads would execute the \textit{printf("Hello world! \textbackslash n");} line and so the message would be displayed only 4 times. We can accomplish this by adding \textit{num\_threads(4)} after \textit{\#pragma omp parallel}. Another way would be by using \textit{export OMP\_NUM\_THREADS=4} before the execution of \textit{1.hola}, so that only 4 threads are available.

\subsubsection{2.hello.c}
\textbf{1. Is the execution of the program correct? (i.e., prints a sequence of \textit{(Thid) Hello (Thid)
world!} being Thid the thread identifier). If not, add a data sharing clause to make it correct?} 

It is not correct, sometimes errors like \\
\textit{(2) Hello (1) Hello (1) world! (1) world!} \\
occur. These happen because the variable \textit{id} is declared before \textit{\#pragma omp parallel}, meaning all threads share it. Then what can happen is that for example thread 2 reads the variable \textit{id}, prints the \textit{"(2) Hello"} line, then thread 1 changes the variable, and when it is time to print the \textit{"(2) world!"}, thread 2 prints \textit{"(1) world!"} instead, because thread 1 assigned a new value before thread 2 was done.
\\
To make it correct, we can add the \textit{private(id)} tag to the \textit{\#pragma omp parallel num\_threads(8)} line, so that every thread has its own local value of the variable \textit{id}. This way, when they assign their own id, they do not change the value other threads are reading from the var \textit{id}.

\textbf{2. Are the lines always printed in the same order? Why the messages sometimes appear intermixed?
(Execute several times in order to see this).}
No, the lines are not always printed in the same order. This happens because threads do not get tasks assigned by \#id order, so whichever thread gets the task earlier in that execution will print earlier.
\\
Messages appear intermixed because even though we added the \textit{private(id)} tag, threads do not wait for other threads to finish their execution before printing their lines (that would be sequential instead of parallel). This way, one thread might print \textit{(id) Hello} and right then another thread's execution might start, printing \textit{(id) Hello} again with a different \textit{id}. After that, they both will finish with \textit{(id) world!} and messages will have been intermixed. To avoid this, we should make the execution of the other threads stop when one thread starts to print until it finishes (we could use the \textit{critical} construct), but that would make the execution sequential (with the added overhead of parallelization, even worse).

\subsubsection{3.how\_many.c}
Assuming the \textit{OMP NUM THREADS} variable is set to 8 with \textit{export OMP NUM THREADS=8}

\textbf{1. How many \textit{Hello world ...} lines are printed on the screen?}

20 \textit{Hello world ...} lines are printed on the screen. Each line is printed as many times as the number of threads available for the region of that line.

\begin{itemize}
    \item Hello world from the first parallel (8)!: 8 times
    \item Hello world from the second parallel (2)!: 2 times
    \item Hello world from the second parallel (3)!: 3 times
    \item Hello world from the third parallel (4)!: 4 times
    \item Hello world from the fourth parallel (3)!: 3 times 
\end{itemize}

\textbf{2. What does omp get num threads return when invoked outside and inside a parallel region?}

Outside a parallel region it returns the number of threads during the sequential execution, which is always just one.
\\
Inside a parallel region however, it returns the number of threads available (busy or not) for that region:
\begin{enumerate}
    \item For the first parallel region it returns all threads available for the program which are 8 (because of the \textit{export OMP\_NUM\_THREADS=8} restriction).
    \item  The second parallel region is inside a for loop which completes 2 iterations with \textit{i = 2 and 3}. Since the number of threads is set to \textit{i} at each iteration by calling \textit{omp\_set\_num\_threads(i)}, in the first iteration \textit{omp\_get\_num\_threads()} returns 2 and in the second and final one it returns 3. \\ Note that \textit{omp\_set\_num\_threads()} overrides the number of threads set by \textit{export OMP\_NUM\_THREADS=8}, which means that for the rest of the execution, unless otherwise specified, the number of available threads will be 3 (because the last call to \textit{omp\_set\_num\_threads(i)} was made in the second iteration, with \textit{i = 3}.)  
    \item Before the third parallel region, the \textit{\#pragma omp parallel} directive is extended by adding \textit{num\_threads(4)}, which overrides the number of available threads just for this entire parallel region to 4. This way \textit{omp\_get\_num\_threads()} returns 4.
    \item For this fourth parallel region the number of threads is not overriden, so it takes the value 3 (which is what \textit{omp\_get\_num\_threads()} returns) from the \textit{omp\_set\_num\_threads(3)} call in the for loop.
\end{enumerate}

\subsubsection{4.data\_sharing.c}
\textbf{1. Which is the value of variable x after the execution of each parallel region with different datasharing
attribute (shared, private, firstprivate and reduction)? Is that the value you would expect? (Execute several times if necessary)}

\begin{enumerate}
    \item After first parallel (shared) x's value keeps varying depending on the execution. It should be 120 as intended by the code because there are 16 threads (as set by \textit{omp\_set\_num\_threads(16)}) and $\sum_{i=0}^{15} i = 120$ , but its value after the execution is usually between 100 and 120. This variation occurs because all threads are sharing the same variable x, and to execute \textit{x+=omp\_get\_thread\_num();} they must first read the value of x and then write the result of the sum to it. When they do this, however, it can happen that for example thread 3 reads the value 10 in x, then thread 1 also reads the value 10, after that thread 3 updates x with 10+3=13 and thread 1, that has the old value of x, finally updates x with the value it knows: 10+1=11. And so x instead of being 10+3+1=14, is 11 because thread 1 did not notice thread 3's update of x. This happens quite randomly depending on the order of tasks assigned and threads occupation, so the final value of x keeps changing execution after execution.
    \item After second parallel (private) x's value is always 5. This is expected as x is assigned the value of 5 right before the second parallel region. Then at the time of declaration of the parallel region, x is declared private (\textit{\#pragma omp parallel private(x)}), which means that each thread will modify a local copy (to each thread) of x, but will in no case modify the shared value of the variable (private(x) also makes the local copies of x start without an initialised value instead of the value of x previous to the parallel region (5)). This way threads can't modify the value other threads have of x, but this also means that after the parallel region, unless strictly specified, the shared value of x is still the same as before executing the region, because none of the threads has written the value of their local copy of x to the shared variable x. This is why in the end of the second parallel region x is always equal to 5.
    \item After third parallel (firstprivate) x's value is also always 5. This is expected as well, since firstprivate(x) has the same behaviour as private(x) with the exception that firstprivate(x) initializes each local copy of x for each thread with the value that x had before the start of the parallel region. In this case, that value is 5. Still, this does not affect the value of x after the execution of the parallel region, but it changes the behaviour inside.
    \item After fourth parallel (reduction) x's value is always 125. This is correct because \textit{pragma omp parallel reduction(+:x)} is supposed to make local copies of x for each thread, just like private, but initializing them to the neutral element (for addition, this would be 0). Then each thread executes its instructions inside the parallel region and after that all local copies' values are added together with the initial value of x (which was 5). This is therefore equivalent to computing the following (remember there are 16 threads): 
    \begin{equation}
        5 + \sum_{i=0}^{15} i = 125
    \end{equation}
    And 125 is indeed what is printed after each execution for the fourth parallel region, so it is safe to assume it works as intended.
    
\end{enumerate}

\subsection{Loop parallelism}
\subsubsection{1.schedule.c}
\textbf{1. Which iterations of the loops are executed by each thread for each schedule kind?}
\begin{itemize}
\item \textbf{Schedule(static)}: There is an equal distribution of iterations for each thread. As we can see in the outputed execution of the loop, each of the four threads gets a chunk of 3 iterations, having in total of 12 iterations. 
\\
\begin{lstlisting}[frame=single]
Going to distribute 12 iterations with schedule(static) ...
Loop 1: (0) gets iteration 0
Loop 1: (0) gets iteration 1
Loop 1: (0) gets iteration 2
Loop 1: (1) gets iteration 3
Loop 1: (1) gets iteration 4
Loop 1: (1) gets iteration 5
Loop 1: (3) gets iteration 9
Loop 1: (3) gets iteration 10
Loop 1: (3) gets iteration 11
Loop 1: (2) gets iteration 6
Loop 1: (2) gets iteration 7
Loop 1: (2) gets iteration 8
\end{lstlisting}

\item \textbf{Schedule(static,2)}: There is also an equal distribution of iterations for each thread but in this case each thread is executing only 2 consecutive iterations of the for loop each time according to the thread order.
As we can see in the outputed execution of the loop, the iterations are given in chunks of size 2 and in order: thread 0 takes iteration 1 and 2, thread 1 takes iteration 2 and 3,...
\\
\begin{lstlisting}[frame=single]
Going to distribute 12 iterations with schedule(static, 2) ...
Loop 2: (3) gets iteration 6
Loop 2: (0) gets iteration 0
Loop 2: (0) gets iteration 1
Loop 2: (1) gets iteration 2
Loop 2: (1) gets iteration 3
Loop 2: (1) gets iteration 10
Loop 2: (1) gets iteration 11
Loop 2: (3) gets iteration 7
Loop 2: (0) gets iteration 8
Loop 2: (0) gets iteration 9
Loop 2: (2) gets iteration 4
Loop 2: (2) gets iteration 5
\end{lstlisting}


\item \textbf{Schedule(dynamic,2)}: Each thread is executing 2 consecutive iterations of the loop each time. However, the order is guided by the first thread that is free. 
As we can see in the outputed execution of the loop, the iterations are given also in chunks of size 2, but here they are not sorted, the assignment is given depending on the availability of the thread. 
\\
\begin{lstlisting}[frame=single]
Going to distribute 12 iterations with schedule(dynamic, 2) ...
Loop 3: (1) gets iteration 0
Loop 3: (1) gets iteration 1
Loop 3: (1) gets iteration 8
Loop 3: (1) gets iteration 9
Loop 3: (1) gets iteration 10
Loop 3: (1) gets iteration 11
Loop 3: (0) gets iteration 6
Loop 3: (0) gets iteration 7
Loop 3: (3) gets iteration 4
Loop 3: (3) gets iteration 5
Loop 3: (2) gets iteration 2
Loop 3: (2) gets iteration 3
\end{lstlisting}

\item \textbf{Schedule(guided,2)}: In this case, the size of each chunk is proportional to the number of unassigned iterations divided by the number of the threads. Therefore the size of the chunks decreases. The order is also given by the first who is free and the chunk-size specified (in this case 2) represents the minimum size of a chunk. 
\\
\begin{lstlisting}[frame=single]
Going to distribute 12 iterations with schedule(guided, 2) ...
Loop 4: (3) gets iteration 0
Loop 4: (3) gets iteration 1
Loop 4: (3) gets iteration 8
Loop 4: (0) gets iteration 2
Loop 4: (0) gets iteration 3
Loop 4: (0) gets iteration 10
Loop 4: (0) gets iteration 11
Loop 4: (1) gets iteration 6
Loop 4: (1) gets iteration 7
Loop 4: (2) gets iteration 4
Loop 4: (2) gets iteration 5
Loop 4: (3) gets iteration 9
\end{lstlisting}
\end{itemize}


\subsubsection{2.nowait.c}
\textbf{1. Which could be a possible sequence of printf when executing the program?}
\\
The nowait clause removes the synchronization time needed between threads. If we remove the clause, the threads don't wait the others to finish their parallel region executions so they continue executing code. That means that without the nowait clause the sequence of printf is printed randomly by both for loops (but respecting the code sequence in a thread). 
Let's see an example of the outputed execution (we changed the number of iterations of the code in order to be able to check our hypothesis): 
\\
\begin{lstlisting}[frame=single]
Loop 1: thread (0) gets iteration 0
Loop 1: thread (4) gets iteration 1
Loop 1: thread (7) gets iteration 2
Loop 2: thread (6) gets iteration 6
Loop 2: thread (5) gets iteration 7
Loop 1: thread (2) gets iteration 3
Loop 2: thread (3) gets iteration 4
Loop 2: thread (1) gets iteration 5

\end{lstlisting}

\textbf{2. How does the sequence of printf change if the nowait clause is removed from the first for directive?}
\\
If we did not have the nowait clause we would see first the sequence of printf done by the first loop and then the sequence done by the second loop:
In order to check if our hypothesis is correct let's see the outputed execution: 
\\
\begin{lstlisting}[frame=single]
Loop 1: thread (1) gets iteration 1
Loop 1: thread (0) gets iteration 0
Loop 2: thread (0) gets iteration 3
Loop 2: thread (1) gets iteration 2
\end{lstlisting} 
\textbf{3. What would happen if dynamic is changed to static in the schedule in both loops? (keeping the nowait clause)}
\\
 Even though we kept the nowait clause we would still get a correct printf sequence: first the first loop and then the second one. But why? The static clause creates a static scheduling type, that is, the iterations are grouped into chunks and each thread executes the threads in circular order. So there is implicitly an order into the static schedule. 

If we check the ouputed execution we can see that out hypothesis is true. 
\\
\begin{lstlisting}[frame=single]
Loop 1: thread (0) gets iteration 0
Loop 1: thread (3) gets iteration 3
Loop 1: thread (2) gets iteration 2
Loop 1: thread (1) gets iteration 1
Loop 2: thread (0) gets iteration 4
Loop 2: thread (2) gets iteration 6
Loop 2: thread (1) gets iteration 5
Loop 2: thread (3) gets iteration 7
\end{lstlisting}

\subsubsection{3.collapse.c}
\textbf{1. Which iterations of the loop are executed by each thread when the collapse clause is used?}
\\
The collapse clause will parallelize more than one level of the loop. So each thread will execute consecutively a set of consecutive iterations that corresponds to the sequential iterations of the loops combined. To sum up, the iterations are in groups like it was only one loop. 
\\
\begin{lstlisting}[frame=single]
(0) Iter (0 0)
(0) Iter (0 1)
(0) Iter (0 2)
(0) Iter (0 3)
(1) Iter (0 4)
(1) Iter (1 0)
(1) Iter (1 1)
(3) Iter (2 0)
(3) Iter (2 1)
(3) Iter (2 2)
...
\end{lstlisting}
\textbf{2. Is the execution correct if the collapse clause is removed? Which clause (different than
collapse) should be added to make it correct?}
\\
If we delete the clause the execution is not correct because only the i is protected and the j is shared. In order to solve it we could make private the variable j to avoid having different iterations of the loop overriding the variable j.
\\
\begin{lstlisting}[frame=single]
int main()
{
    int i,j;

    omp_set_num_threads(8);
    #pragma omp parallel for private(j)
    for (i=0; i < N; i++) {
          for (j=0; j < N; j++) {
          int id=omp_get_thread_num();
          printf("(%d) Iter (%d %d)\n",id,i,j);
        }
    }

    return 0;
}

\end{lstlisting}
 
\subsection{Synchronization}
\subsubsection{1.datarace.c} 
\textbf{1. Is the program always executing correctly?}
No. There is a data race.
\textbf{2. Add two alternative directives to make it correct. Explain why they make the execution correct.}
\\
\textbf{Alternative 1}
\\
\begin{lstlisting}[frame=single]
int main()
{

    int i, x=0;
    omp_set_num_threads(8);
    #pragma omp parallel private(i)
    {
    	int id = omp_get_thread_num();
    	for (i=id; i < N;i+=8) {
        	#pragma omp critical
        	x++;
    	}
	}

    if (x==N) printf("Congratulations!, program executed correctly (x = %d)\n", x);
    else printf("Sorry, something went wrong, value of x = %d\n", x);

    return 0;
}  
\end{lstlisting}  
This version protects the operation x++, where we had the data race. We created a critical region that only one thread can execute at once.
\\
\textbf{Alternative 2}
\\
\begin{lstlisting}[frame=single]
int main()
{

    int i, x=0;
    omp_set_num_threads(8);
    #pragma omp parallel private(i)
     {
    	int id = omp_get_thread_num();
   		for (i=id; i < N;i+=8) {
        	#pragma omp atomic
        	x++;
    	}
	}

    if (x==N) printf("Congratulations!, program executed correctly (x = %d)\n", x);
    else printf("Sorry, something went wrong, value of x = %d\n", x);

    return 0;
}
\end{lstlisting}
Like in the previous version, here we are also protecting the operation x++, in this case we made the operation x++ indivisible using the atomic pragma directive, forcing to make the read and write operations on one thread before allowing another thread to access the variable. 

\subsubsection{2.barrier.c}
\textbf{1. Can you predict the sequence of messages in this program? Do threads exit from the barrier
in any specific order?} 

FALTA FER 

\subsubsection{3.ordered.c}
\textbf{1.Can you explain the order in which the \textit{Outside} and \textit{Inside} messages are printed?} 
\\
The orderer clause works like this: different threads execute concurrently until they encounter the ordered region, which is executed sequentially in the same order as it would get executed in a serial loop. So, the order in which the \textit{Outside} and \textit{Inside} messages are printed is because of the fact that the inside message is inside the ordered region (that is executed sequentially) and the outside message is outside that region where all the different threads are executing concurrently. 
\\
\begin{lstlisting}[frame=single]
Before ordered - (0) gets iteration 0
Inside ordered - (0) gets iteration 0
Before ordered - (4) gets iteration 1
Inside ordered - (4) gets iteration 1
Before ordered - (4) gets iteration 3
Before ordered - (0) gets iteration 2
Inside ordered - (0) gets iteration 2
Before ordered - (2) gets iteration 5
Before ordered - (3) gets iteration 4
Before ordered - (5) gets iteration 6
Before ordered - (0) gets iteration 7
Inside ordered - (4) gets iteration 3
Before ordered - (6) gets iteration 10
Before ordered - (7) gets iteration 9
Before ordered - (1) gets iteration 8
Inside ordered - (3) gets iteration 4
Inside ordered - (2) gets iteration 5
Before ordered - (2) gets iteration 13
...
\end{lstlisting}
\textbf{2. How can you ensure that a thread always executes two consecutive iterations in order during the execution of the ordered part of the loop body?}
\\
Changing from schedule dynamic to schedule(dynamic,2)

\subsection{Tasks}
\subsubsection{1.single.c}
\textbf{1. Can you explain why all threads contribute to the execution of instances of the single worksharing
construct? Why are those instances appear to be executed in bursts?}
\subsubsection{2.fibtask.c}
\textbf{1. Why all tasks are created and executed by the same thread? In other words, why the program
is not executing in parallel?}
\textbf{2. Modify the code so that the program correctly executes in parallel, returning the same answer
that the sequential execution would return.}
\subsubsection{3.synchtasks.c}
\textbf{1. Draw the task dependence graph that is specified in this program}
\textbf{2. Rewrite the program using only taskwait as task synchronisation mechanism (no depend clauses allowed)}
\subsubsection{4.taskloop.c}
\textbf{1. Find out how many tasks and how many iterations each task execute when using the grainsize
and num tasks clause in a taskloop. You will probably have to execute the program several times in order to have a clear answer to this question.}
\textbf{2. What does occur if the nogroup clause in the first taskloop is uncommented?}

\section{Observing overheads}

\section{Conclusion}




\end{document}